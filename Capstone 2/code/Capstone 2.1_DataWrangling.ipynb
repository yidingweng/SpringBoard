{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download nltk stopwords and spacy model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need the stopwords from NLTK and spacy’s en model for text pre-processing. Then we will be using the spacy model for lemmatization. Lemmatization is converting a word to its root word. \n",
    "\n",
    "For example: the lemma of the word ‘machines’ is ‘machine’. \n",
    "Likewise, ‘walking’ –> ‘walk’, ‘mice’ –> ‘mouse’ and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yidingweng/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk; nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already downloaded the stopwords. Let’s import them and make it available in stop_words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import news article data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of 3.csv files. we will concatenate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('data//articles1.csv',index_col=0)\n",
    "data2 = pd.read_csv('data//articles2.csv',index_col=0)\n",
    "data3 = pd.read_csv('data//articles3.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142570, 9)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total = pd.concat([data1, data2, data3])\n",
    "df_total.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version of the dataset contains about 142k news articles from 15 different medias. This is available as https://www.kaggle.com/jannesklaas/analyzing-the-news/data. Due to the limited computing power, I will take a sample of 30% of the total data for our study in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_total.sample(frac=0.3, replace=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>131500</th>\n",
       "      <td>198273</td>\n",
       "      <td>Ivanka Trump just got booed in Germany for cal...</td>\n",
       "      <td>Vox</td>\n",
       "      <td>Zeeshan Aleem</td>\n",
       "      <td>2017/4/25</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>http://www.vox.com/world/2017/4/25/15420358/iv...</td>\n",
       "      <td>Ivanka Trump is having trouble convincing the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5192</th>\n",
       "      <td>23073</td>\n",
       "      <td>At Debate, Hillary Clinton Leaves Questions Ab...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Peter Eavis</td>\n",
       "      <td>2016-04-15</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A jarring regulatory action this week against ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53350</th>\n",
       "      <td>73533</td>\n",
       "      <td>Why Mars Is the Best Planet</td>\n",
       "      <td>Atlantic</td>\n",
       "      <td>Rebecca Boyle</td>\n",
       "      <td>2017-01-13</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our tale of two planets begins four billion ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112720</th>\n",
       "      <td>167391</td>\n",
       "      <td>Sketch To Impress: How An Oscar-Winning Design...</td>\n",
       "      <td>NPR</td>\n",
       "      <td>Mandalit del Barco</td>\n",
       "      <td>2016-02-24</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>http://www.npr.org/2016/02/24/467800435/sketch...</td>\n",
       "      <td>British costumer Sandy Powell already has thre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76783</th>\n",
       "      <td>117088</td>\n",
       "      <td>What to Make of the Saudi Shake-up</td>\n",
       "      <td>National Review</td>\n",
       "      <td>Elliott Abrams</td>\n",
       "      <td>2017-06-21</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>http://www.nationalreview.com/article/448834/s...</td>\n",
       "      <td>On Wednesday, King Salman of Saudi Arabia push...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                              title  \\\n",
       "131500  198273  Ivanka Trump just got booed in Germany for cal...   \n",
       "5192     23073  At Debate, Hillary Clinton Leaves Questions Ab...   \n",
       "53350    73533                        Why Mars Is the Best Planet   \n",
       "112720  167391  Sketch To Impress: How An Oscar-Winning Design...   \n",
       "76783   117088                What to Make of the Saudi Shake-up    \n",
       "\n",
       "            publication              author        date    year  month  \\\n",
       "131500              Vox       Zeeshan Aleem   2017/4/25  2017.0    4.0   \n",
       "5192     New York Times         Peter Eavis  2016-04-15  2016.0    4.0   \n",
       "53350          Atlantic       Rebecca Boyle  2017-01-13  2017.0    1.0   \n",
       "112720              NPR  Mandalit del Barco  2016-02-24  2016.0    2.0   \n",
       "76783   National Review      Elliott Abrams  2017-06-21  2017.0    6.0   \n",
       "\n",
       "                                                      url  \\\n",
       "131500  http://www.vox.com/world/2017/4/25/15420358/iv...   \n",
       "5192                                                  NaN   \n",
       "53350                                                 NaN   \n",
       "112720  http://www.npr.org/2016/02/24/467800435/sketch...   \n",
       "76783   http://www.nationalreview.com/article/448834/s...   \n",
       "\n",
       "                                                  content  \n",
       "131500   Ivanka Trump is having trouble convincing the...  \n",
       "5192    A jarring regulatory action this week against ...  \n",
       "53350   Our tale of two planets begins four billion ye...  \n",
       "112720  British costumer Sandy Powell already has thre...  \n",
       "76783   On Wednesday, King Salman of Saudi Arabia push...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check empty fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                 0\n",
       "title              0\n",
       "publication        0\n",
       "author          4872\n",
       "date             798\n",
       "year             798\n",
       "month            798\n",
       "url            16934\n",
       "content            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'title' and 'content' are the important features that have the text information we are interested to study. Since this title dataset does not have empty fields in 'title' or 'content' information, we will not remove any rows of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#df.dropna(subset=['title'], inplace = True)\n",
    "#df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/allNews_30%sample.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove emails and newline characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there are many emails, newline and extra spaces that is quite distracting. So get rid of them using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "df.content = df.content.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id                                              title publication  \\\n",
      "131500  198273  Ivanka Trump just got booed in Germany for cal...         Vox   \n",
      "\n",
      "               author       date    year  month  \\\n",
      "131500  Zeeshan Aleem  2017/4/25  2017.0    4.0   \n",
      "\n",
      "                                                      url  \\\n",
      "131500  http://www.vox.com/world/2017/4/25/15420358/iv...   \n",
      "\n",
      "                                                  content  \n",
      "131500   Ivanka Trump is having trouble convincing the...  \n"
     ]
    }
   ],
   "source": [
    "# Remove Emails\n",
    "df.content = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in df.content]\n",
    "\n",
    "# Remove new line characters\n",
    "df.content = [re.sub('\\s+', ' ', sent) for sent in df.content]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "df.content = [re.sub(\"\\'\", \"\", sent) for sent in df.content]\n",
    "\n",
    "pprint(df[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"data/allNews_30%sample_lemmatized_nontokenized.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing the emails and extra spaces, we need to break down each sentence into a list of words through tokenization, while clearing up all the messy text in the process.\n",
    "\n",
    "Gensim’s simple_preprocess is great for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize words and Clean-up text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131500    [ivanka, trump, is, having, trouble, convincin...\n",
      "Name: content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "df.content = list(sent_to_words(df.content))\n",
    "\n",
    "print(df.content[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Bigram and Trigram Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigrams are two words frequently occurring together in the document. Trigrams are 3 words frequently occurring.\n",
    "Gensim’s Phrases model can build and implement the bigrams, trigrams, quadgrams and more. The two important arguments to Phrases are min_count and threshold. The higher the values of these param, the harder it is for words to be combined to bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['washington', 'congressional', 'republicans', 'have', 'new', 'fear', 'when', 'it', 'comes', 'to', 'their', 'health_care', 'lawsuit', 'against', 'the', 'obama', 'administration', 'they', 'might', 'win', 'the', 'incoming', 'trump', 'administration', 'could', 'choose', 'to', 'no', 'longer', 'defend', 'the', 'executive_branch', 'against', 'the', 'suit', 'which', 'challenges', 'the', 'administration', 'authority', 'to', 'spend', 'billions', 'of', 'dollars', 'on', 'health_insurance', 'subsidies', 'for', 'and', 'americans', 'handing', 'house', 'republicans', 'big', 'victory', 'on', 'issues', 'but', 'sudden', 'loss', 'of', 'the', 'disputed', 'subsidies', 'could', 'conceivably', 'cause', 'the', 'health_care', 'program', 'to', 'implode', 'leaving', 'millions', 'of', 'people', 'without', 'access', 'to', 'health_insurance', 'before', 'republicans', 'have', 'prepared', 'replacement', 'that', 'could', 'lead', 'to', 'chaos', 'in', 'the', 'insurance', 'market', 'and', 'spur', 'political', 'backlash', 'just', 'as', 'republicans', 'gain', 'full', 'control', 'of', 'the', 'government', 'to', 'stave_off', 'that', 'outcome', 'republicans', 'could', 'find', 'themselves', 'in', 'the', 'awkward', 'position', 'of', 'appropriating', 'huge_sums', 'to', 'temporarily', 'prop', 'up', 'the', 'obama', 'health_care', 'law', 'angering', 'conservative', 'voters', 'who', 'have', 'been', 'demanding', 'an', 'end', 'to', 'the', 'law', 'for', 'years', 'in', 'another', 'twist', 'donald', 'trump', 'administration', 'worried', 'about', 'preserving', 'executive_branch', 'prerogatives', 'could', 'choose', 'to', 'fight', 'its', 'republican', 'allies', 'in', 'the', 'house', 'on', 'some', 'central', 'questions', 'in', 'the', 'dispute', 'eager', 'to', 'avoid', 'an', 'ugly', 'political', 'pileup', 'republicans', 'on', 'capitol_hill', 'and', 'the', 'trump', 'transition_team', 'are', 'gaming', 'out', 'how', 'to', 'handle', 'the', 'lawsuit', 'which', 'after', 'the', 'election', 'has', 'been', 'put', 'in', 'limbo', 'until', 'at', 'least', 'late', 'february', 'by', 'the', 'united_states', 'court', 'of', 'appeals', 'for', 'the', 'district', 'of', 'columbia_circuit', 'they', 'are', 'not', 'yet', 'ready', 'to', 'divulge', 'their', 'strategy', 'given', 'that', 'this', 'pending_litigation', 'involves', 'the', 'obama', 'administration', 'and', 'congress', 'it', 'would', 'be', 'inappropriate', 'to', 'comment', 'said', 'phillip', 'blando', 'spokesman', 'for', 'the', 'trump', 'transition', 'effort', 'upon', 'taking', 'office', 'the', 'trump', 'administration', 'will', 'evaluate', 'this', 'case', 'and', 'all', 'related', 'aspects', 'of', 'the', 'affordable_care_act', 'in', 'potentially', 'decision', 'in', 'judge_rosemary_collyer', 'ruled', 'that', 'house', 'republicans', 'had', 'the', 'standing', 'to', 'sue', 'the', 'executive_branch', 'over', 'spending', 'dispute', 'and', 'that', 'the', 'obama', 'administration', 'had', 'been', 'distributing', 'the', 'health_insurance', 'subsidies', 'in', 'violation', 'of', 'the', 'constitution', 'without', 'approval', 'from', 'congress', 'the', 'justice_department', 'confident', 'that', 'judge_collyer', 'decision', 'would', 'be', 'reversed', 'quickly', 'appealed', 'and', 'the', 'subsidies', 'have', 'remained', 'in', 'place', 'during', 'the', 'appeal', 'in', 'successfully', 'seeking', 'temporary_halt', 'in', 'the', 'proceedings', 'after', 'mr', 'trump', 'won', 'house', 'republicans', 'last', 'month', 'told', 'the', 'court', 'that', 'they', 'and', 'the', 'transition_team', 'currently', 'are', 'discussing', 'potential', 'options', 'for', 'resolution', 'of', 'this', 'matter', 'to', 'take', 'effect', 'after', 'the', 'inauguration', 'on', 'jan', 'the', 'suspension', 'of', 'the', 'case', 'house', 'lawyers', 'said', 'will', 'provide', 'the', 'and', 'his', 'future', 'administration', 'time', 'to', 'consider', 'whether', 'to', 'continue', 'prosecuting', 'or', 'to', 'otherwise', 'resolve', 'this', 'appeal', 'republican', 'leadership', 'officials', 'in', 'the', 'house', 'acknowledge', 'the', 'possibility', 'of', 'cascading', 'effects', 'if', 'the', 'payments', 'which', 'have', 'totaled', 'an', 'estimated', 'billion', 'are', 'suddenly', 'stopped', 'insurers', 'that', 'receive', 'the', 'subsidies', 'in', 'exchange', 'for', 'paying', 'costs', 'such', 'as', 'deductibles', 'and', 'for', 'eligible', 'consumers', 'could', 'race', 'to', 'drop', 'coverage', 'since', 'they', 'would', 'be', 'losing', 'money', 'over', 'all', 'the', 'loss', 'of', 'the', 'subsidies', 'could', 'destabilize', 'the', 'entire', 'program', 'and', 'cause', 'lack', 'of', 'confidence', 'that', 'leads', 'other', 'insurers', 'to', 'seek', 'quick', 'exit', 'as', 'well', 'anticipating', 'that', 'the', 'trump', 'administration', 'might', 'not', 'be', 'inclined', 'to', 'mount', 'vigorous', 'fight', 'against', 'the', 'house', 'republicans', 'given', 'the', 'dim_view', 'of', 'the', 'health_care', 'law', 'team', 'of', 'lawyers', 'this', 'month', 'sought', 'to', 'intervene', 'in', 'the', 'case', 'on', 'behalf', 'of', 'two', 'participants', 'in', 'the', 'health_care', 'program', 'in', 'their', 'request', 'the', 'lawyers', 'predicted', 'that', 'deal', 'between', 'house', 'republicans', 'and', 'the', 'new', 'administration', 'to', 'dismiss', 'or', 'settle', 'the', 'case', 'will', 'produce', 'devastating', 'consequences', 'for', 'the', 'individuals', 'who', 'receive', 'these', 'reductions', 'as', 'well', 'as', 'for', 'the', 'nation', 'health_insurance', 'and', 'health_care', 'systems', 'generally', 'no', 'matter', 'what', 'happens', 'house', 'republicans', 'say', 'they', 'want', 'to', 'prevail', 'on', 'two', 'overarching', 'concepts', 'the', 'congressional', 'power', 'of', 'the', 'purse', 'and', 'the', 'right', 'of', 'congress', 'to', 'sue', 'the', 'executive_branch', 'if', 'it', 'violates', 'the', 'constitution', 'regarding', 'that', 'spending', 'power', 'house', 'republicans', 'contend', 'that', 'congress', 'never', 'appropriated', 'the', 'money', 'for', 'the', 'subsidies', 'as', 'required', 'by', 'the', 'constitution', 'in', 'the', 'suit', 'which', 'was', 'initially', 'championed', 'by', 'john_boehner', 'the', 'house', 'speaker', 'at', 'the', 'time', 'and', 'later', 'in', 'house', 'committee', 'reports', 'republicans', 'asserted', 'that', 'the', 'administration', 'desperate', 'for', 'the', 'funding', 'had', 'required', 'the', 'treasury_department', 'to', 'provide', 'it', 'despite', 'widespread', 'internal', 'skepticism', 'that', 'the', 'spending', 'was', 'proper', 'the', 'white_house', 'said', 'that', 'the', 'spending', 'was', 'permanent', 'part', 'of', 'the', 'law', 'passed', 'in', 'and', 'that', 'no', 'annual', 'appropriation', 'was', 'required', 'even', 'though', 'the', 'administration', 'initially', 'sought', 'one', 'just', 'as', 'important', 'to', 'house', 'republicans', 'judge_collyer', 'found', 'that', 'congress', 'had', 'the', 'standing', 'to', 'sue', 'the', 'white_house', 'on', 'this', 'issue', 'ruling', 'that', 'many', 'legal', 'experts', 'said', 'was', 'flawed', 'and', 'they', 'want', 'that', 'precedent', 'to', 'be', 'set', 'to', 'restore', 'congressional', 'leverage', 'over', 'the', 'executive_branch', 'but', 'on', 'spending', 'power', 'and', 'standing', 'the', 'trump', 'administration', 'may', 'come', 'under', 'pressure', 'from', 'advocates', 'of', 'presidential', 'authority', 'to', 'fight', 'the', 'house', 'no', 'matter', 'their', 'shared', 'views', 'on', 'health_care', 'since', 'those', 'precedents', 'could', 'have', 'broad', 'repercussions', 'it', 'is', 'complicated', 'set', 'of', 'dynamics', 'illustrating', 'how', 'quick', 'legal', 'victory', 'for', 'the', 'house', 'in', 'the', 'trump', 'era', 'might', 'come', 'with', 'costs', 'that', 'republicans', 'never', 'anticipated', 'when', 'they', 'took', 'on', 'the', 'obama', 'white_house']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(df.content, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[df.content.tolist()], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[df.content[0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples in our example are: ‘health_insurance’, ‘health_care’, ‘white_house’ etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stopwords, Make Bigrams and Lemmatize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bigrams model is ready. Let’s define the functions to remove the stopwords, make bigrams and lemmatization and call them sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "df.content = remove_stopwords(df.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "df.content = lemmatization(df.content, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"data/allNews_30%sample_lemmatized_nonbigrams.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131500    [ivanka, trump, trouble, convince, world, fath...\n",
      "Name: content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Form Bigrams\n",
    "df.content = make_bigrams(df.content)\n",
    "\n",
    "print(df.content[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"data/allNews_30%sample_lemmatized.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
